*基本概念：
 · npm多次升级的核心问题是解决包的版本管理问题
 · 包的多版本共存问题：
   ~ 通常包都允许多个版本同时存在于项目中
   ~ 部分包本身就不支持多版本共存，例如core-js会污染全局环境
 · 什么是依赖地狱？
   在软件管理的领域里存在着被称作“依赖地狱”的死亡之谷，系统规模越大，加入的套件越多，你就越有可能在未来的某一天发现自己已深陷绝望之中。在依赖高的系统中发布新版本套件可能很快会成为恶梦。如果依赖关系过高，可能面临版本控制被锁死的风险（必须对每一个相依套件改版才能完成某次升级）。而如果依赖关系过于松散，又将无法避免版本的混乱（假设兼容于未来的多个版本已超出了合理数量）。当你专案的进展因为版本相依被锁死或版本混乱变得不够简便和可靠，就意味着你正处于依赖地狱之中。
 · 依赖地狱的特点：
   ~ 依赖过多：一个软件包依赖于众多的库，因此安装一个软件包的同时要安装几个甚至几十个库包
   ~ 多重依赖：指从所需软件包到最底层软件包之间的层级数过多。这会导致依赖性解析过于复杂，并且容易产生依赖冲突和环形依赖
   ~ 依赖冲突：即两个软件包无法共存的情况。除两个软件包包含内容直接冲突外，也可能因为其依赖的低层软件包互相冲突。因此，两个看似毫无关联的软件包也可能因为依赖性冲突而无法安装
   ~ 依赖循环：即依赖性关系形成一个闭合环路，最终导致：在安装A软件包之前，必须要安装A、B、C、D软件包，然而这是不可能的
 · node解决依赖地狱的策略：
   ~ node的解决方式是依赖的node加载模块的路径查找算法和node_modules的目录结构来配合解决的
   ~ 模块的路径查找算法有2个核心：
      ~ 优先读取最近的node_modules的依赖
      ~ 递归向上查找node_modules依赖
   ~ 该算法虽然简化了Dependency hell的解决方式，也带来了非常多的问题



*模块路径查找算法+node_modules目录结构的缺陷：
 · nest mode：
   ~ 该策略是依赖谁就安装谁
   ~ npm2采用的该策略对包进行管理
   ~ 该方案的node_modules结构如下：
     mod-a → mod-b@1.0
     mod-c → mod-b@2.0
     mod-d → mod-b@2.0
     解析：
       mod-a就近的使用mod-b的1.0版本，而mod-c就近的使用了mod-b的2.0版本。但是这样带来了另一个问题，如果我们此时再依赖一个mod-d,该mod-d也同时依赖的mod-b的2.0版本，结果是mod-b@2.0安装了2遍。
       如果你的应用了很多的第三方库，同时第三方库共同依赖了一些很基础的第三方库如lodash，你会发现你的node_modules里充满了各种重复版本的lodash,，这既是臭名昭著的node_modules hell
   ~ 缺点：
      ~ 依赖树的层级可能会非常深。如果要定位某依赖的依赖，很难找到该依赖的文件所在
      ~ 层级太深还会导致删除失败
      ~ 不同的依赖书分支中，可能有大量实际上是同版本的依赖
      ~ 安装时额外下载或者拷贝了大量重复的资源，造成了极大的空间浪费
      ~ 安装速度慢
 · flat mode：
   ~ 该策略利用向上递归查找依赖的特性，将一些公共依赖放在公共的node_module里
   ~ npm3采用的该策略对包进行管理
   ~ 该方案的node_modules结构如下：
     mod-a → mod-b@1.0
     mod-c
     mod-d
     mod-b@2.0
     解析：
       模块a、c、d、b@2.0放在同一层，当模块c、d在下级目录中找不到b@2.0将会向上找到mod-b@2.0模块。
       该策略可以避免模块重复安装，节省空间和时间 
   ~ 缺点：
      ~ doppelgangers:
        当我们再引入一个模块e，并且它依赖b@1.0时，node_modules结构如下：
        mod-a → mod-b@1.0
        mod-c
        mod-d
        mod-e → mod-b@1.0
        mod-b@2.0
        解析：
          我们发现无论是把b@2.0还是b@1.0放在top level,都会导致另一个版本任何会存在重复的问题。
          doppelgangers导致的重复问题，除了浪费空间和加长安装时间，还会导致另一个小问题：全局types冲突。
         ~ 全局types冲突：
           虽然各个package之前的代码不会相互污染，但是他们的types仍然可以相互影响，很多的第三方库会修改全局的类型定义。
           典型的就是@types/react,例如出现：‘Duplicate identifier 'FormData'’，错误原因就在于全局的types形成了命名冲突，因此假如版本重复可能会导致全局的类型错误。
           一般的解决方式就是自己控制包含哪些加载的@types/xxx
      ~ phantom depdency：
         ~ 把一个库使用了不属于其depdencies里的package称之为 phantom depdencies
         ~ 实例：例如当我们在开发环境中安装rimraf包时，即使glob和brace-expansion包都在depdencies里，我们仍能在代码使用glob和brace-expansion，因为他们两个都是rimraf的依赖，在被复用的情况下放到了公共node_module中。一旦将该库发布，因为用户安装我们的库的时候并不会安装库的devDepdency，这导致在用户的地方会运行报错。
      ~ 在基于yarn或者npm的node_modules的结构下，doppelganger和phantom dependency似乎并没有太好的解决方式。其本质是因为npm和yarn通过node resolve算法配合node_modules的树形结构对原本depdency graph的模拟
 · Semver：
   ~ Semver：语义化的版本控制
   ~ 产生背景：当项目引入的依赖越来越多时，很容易导致依赖地域的产生。而Semver是解决地狱依赖的有效方式之一
   ~ semver的工作原理：
     Semver是一组简单的规则及条件来约束版本号的配置和增长。这些规则是根据（但不局限于）已经被各种封闭、开放源码软件所广泛使用的惯例所设计。为了让这套理论运作，你必须先有定义好的公共API。这可以透过文件定义或代码强制要求来实现。无论如何，这套API的清楚明了是十分重要的。一旦你定义了公共API，你就可以透过修改相应的版本号来向大家说明你的修改。考虑使用这样的版本号格式：XYZ（主版本号.次版本号.修订号）修复问题但不影响API时，递增修订号；API保持向下兼容的新增及修改时，递增次版本号；进行不向下兼容的修改时，递增主版本号。
     如果你为你的每一个版本都写死依赖，那么如果某个底层的依赖需要修复或者升级，你难以评估这个升级会修复的影响范围，这可能导致级联反应，与其协作的任何包都可能会挂掉，导致整个系统都需要全量的测试回归，最后的结果很大可能是整个应用彻底锁死版本，再也不敢做任何升级改动。
     因此semver的提出主要是用于控制每个package的影响范围，能够实现系统的平滑升级和过渡，npm每次安装都会按照semver的限制，安装最新的符合约束的依赖。
   ~ 当semver遇到现实：如果所有的库都能完美的遵守语义化版本，那么地狱依赖问题能够很好的解决掉。然而现实是很多库因为种种原因并未遵守semver，原因包括：
      ~ 不可预知的bug，本来以为某个版本只是bug fix，发布了patch版本，但是该patch却引入了未预料的breaking change（不兼容改版）导致semver被破坏
      ~ semver的设计过于理想，实际上即使是最小的bug fix，如果业务方无意中依赖了这个bug，仍然会导致breaking change，bug和breaking change的界限是模糊的
      ~ 认为semver没有太大意义，例如Typescript官方就承认从未遵循semver语义，实际上typescript经常在minor版本引入各种breaking change
 · lock： 
   ~ semver的破坏导致依赖差异：
     当部分依赖不遵守semver时，测试时候的代码和上线的代码可能并不是完全一致的，导致项目上线出现问题
   ~ 写死版本不一定奏效：
     为保证项目的依赖一致性，首先想到的办法是直接把依赖的第三方版本都写死。然而问题并没这么简单，例如虽然你锁定了webpack的版本，但是webpack的依赖却没法锁定，如果webpack的某个依赖法生产不遵循semver的breaking change，我们的应用还是会受到影响，除非我们保证所有的第三方以及第三方的依赖都是写死版本，这即意味着整个社区放弃semver，这显然是不可能的。
   ~ yarn lock vs npm lock：一个更加靠谱的写法是将项目里的依赖和第三方的依赖同时锁定，yarn的lock和npm的lock都支持该功能。
   ~ npm5采用的该策略对包进行管理，安装时默认产生lock文件
   ~ lock也不是万能的： 
     当我们第一次安装创建项目时或者第一次安装某个依赖的时候，此时即使第三方库里含有lock文件，但是npm install|(yarn install) 并不会去读取第三方依赖的lock，这导致第一次创建项目的时候，用户还是会可能触发bug。这在全局安装cli的场景下非常常见，经常会碰到上一次安装全局cli的时候正常，但是重新安装这个版本的cli却挂了，这很有可能是该cli的版本的某个上游依赖发生了breaking change,因为不存在全局环境的lock
   ~ shrinkwrap：
      ~ shrinkwrap是初次安装就出现依赖breaking change的解决方案
      ~ 如果你某天安装了一个新的webpack-cli，却发现这个webpack-cli并不能正常工作，经过一番定位发现，是该cli的一个上游依赖portfinder的最近一个版本有bug，但是该cli的作者在休假，没办法及时修复这个cli，但项目赶着上线该怎么处理？yarn提供了一个叫做[https://classic.yarnpkg.com/en/docs/selective-version-resolutions/](https://classic.yarnpkg.com/en/docs/selective-version-resolutions/)的机制，使得你可以忽略dependency的限制，强行将portfinder锁定为某个没有bug的版本，以解燃眉之急。npm可以使用npm shrinkwrap实现该机制
      ~ npm shrinkwrap时会生成npm-shrinkwrap.json文件，该文件将可以将所有层级依赖的版本进行锁定，保证所有曾经依赖版本完全一致
      ~ npm shrinkwrap也不是万能药，它将导致版本锁定而不敢升级，具体讨论看这里：《为什么我不使用 shrinkwrap（lock）》：https://zhuanlan.zhihu.com/p/22934066
   ~ 库里应该提交lock文件吗？
     前面提到npm和yarn在install的时候并不会读取第三方库里的lock文件，那么我们编写库的时候还有必要提供lock文件吗。不知道大家有没有过这种经验，某天发现了某个第三方库存在某个bug，摩拳擦掌的将该库下载下来，准备修复下发个mr，一顿npm install && npm build 操作猛如虎，然后就见到了一堆莫名其妙的编译错误，这些错误很可能个是编译工具的某个上游依赖的breaking change所致，经过一番google + stackoverflow仍然没有修复，这时候就基本上断了提mr的冲动，如果库的开发者将当前的编译环境的lock提交上来，则很大程度上可以避免该问题。
   ~ determinism：
      ~ determinism指的是在给定package.json和lock文件下，每次重新install都会得到同样的node_modules的拓扑结构。
      ~ 版本确定性 !== 拓扑确定性
      ~ 事实上yarn仅保证了同一版本的确定性而无法保证不同版本的确定性，npm则保证了不同版本的确定性。
         ~ yarn.lock保证了所有第三方库和其依赖的版本号是锁定的，虽然保证了版本，但是实际上yarn.lock里并没有包含任何的node_modules拓扑信息
         ~ npm的lock信息则包含了拓扑结构信息


*文档：
 · 《npm install 原理分析》：https://blog.csdn.net/liuyan19891230/article/details/103856130
 · 《node_modules 困境》：https://zhuanlan.zhihu.com/p/137535779









